[TOC]

# 1. Linux+Shell

## 1.1 常用的高级命令

top

iotop

df -h

grep 

sed 

awk

netstat

halt

ps

find

## 1.2 查看进程、查看端口号、查看磁盘使用情况

top  ps netstat df -h

## 1.3 sed awk cut sort

只需要记住名称即可，最好总结一下这些命令的用法

## 1.4 写过哪些shell脚本

### 1.4.1 集群启停脚本

```bash
#! /bin/bash
case $1 in
"start"){
	for i in Hadoop02 Hadoop03 Hadoop04
	do
		ssh $i "绝对路径 start"
	done

};;
"stop"){

};;
```

### 1.4.2 数仓层级导入

ods=>dwd=>dws

```bash
#! /bin/bash

#定义变量
hive=/opt/module/hive/bin/hive
APP=gmall
#定义时间

sql = "

遇到表加上数据库名称
遇到时间，$do_data

"

hive -e "$sql"
```

### 1.4.3 数仓与MySQL的导入导出



## 1.5 单引号和双引号的区别

双引号可以取出里面变量的值，单引号不可以



# 2. Hadoop

## 2.1 入门

### 2.1.1 端口号

**Hadoop2系列的端口号:**

50070:查看HDFS运行情况

8088:查看任务运行情况

19888:历史服务器

9000:外部访问集群的一个通信端口



**Hadoop2系列的端口号:**

50070 -> 9870

9000 -> 8020

### 2.1.2 配置文件

**4个common**

core-site.xml

hdfs-site.xml

yarn-site.xml

mapred-site.xml

**3个env**

slaves(Hadoop3变更为workers) 该文件不能有空行、空格

## 2.2 HDFS

### 2.2.1 读写流程(笔试题)





### 2.2.2 小文件

+ 危害

  **NameNode的内存不够**

  128G内存能够存储的文件块数量为128\*1024\*1024 / 150字节 = 9亿

  **计算**

  一个文件对应一个切片，对应一个mapTask,容易资源耗尽

+ 解决

  har归档、自定义Inputformat > 减少NameNode内存

  使用CombineTextInputformat > 减少切片数 > 减少内存数

  JVM重用



### 2.2.3 块大小、副本数

128m Hadoop2默认

64m Hadoop1默认

32m 本地默认

256m hive默认 大厂

## 2.3 MapReduce(shuffle+优化)

Map方法之后，reduce方法之前

画图

MapReduce过程中哪些过程可以进行压缩



内存分配

![image-20210208110352576](/Users/cuiguangsong/go/src/docs/BigData/面试/images/1.png)

## 2.4 Yarn

### 2.4.1 工作机制



### 2.4.2 调度器

+ FIFO调度器

+ 容量调度器

+ 公平调度器

**默认调度器：**

Apache:容量调度器

cdh:公平调度器

每个调度器的特点

FIFO:单队列；先进先出，在企业里面不使用

容量调度器:支持多队列，跨队列借用资源，并发度较高，一般在中小企业中使用

公平调度器:支持多队列，跨队列借用资源，并发度最高，一般在中大型公司使用



**在企业中怎么配置任务队列**

容量调度器默认只有一个default

按照框架名称配置队列:hive/spark/flink/tez

按照业务名称配置队列:购物车、订单、注册



# 3. Zookeeper

## 3.1 安装台数

安装奇数台

10台服务器：3

20台服务器：5

50台服务器:7

100台服务器:11

台数:

​	过少：影响可靠性

​	过多：影响通信时间

## 3.2 选举机制

半数机制



## 3.3 常用命令

ls create get



# 4. flume

## 4.1 组成

+ source 

  TaildirSource好处:

  断点续传、多目录

  

  哪个版本产生的上述特点 Apache 1.7 

  

  没出现之前taildir之前怎么做的：自定义source

  

  挂了怎么办？有可能重复数据、不会丢数

  

  如果产生了重复数据，怎么办？

  不处理:在企业里面很多情况就不处理，会影响性能

  处理:增加事务

  ​         在下一级处理: hive dwd层；SparkStreaming  redis

  ​          具体的处理手段:开窗取第一条或者分组

  

  是否支持递归

  默认不支持，如果想要支持递归需要自定义代码(递归代码+读取文件代码)

  

  

+ channel

  file channel:基于磁盘，传输慢，可靠性高

  Memory channel：基于内存，传输快，可靠性差

  Kafka channel：数据存储在kafka,基于磁盘，性能优于memory channel+ kafka sink

+ sink

+ 事务

+ 

  



## 4.2 3个器



## 4.3 优化








