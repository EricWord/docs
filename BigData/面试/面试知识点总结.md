[TOC]

# 1. Linux+Shell

## 1.1 常用的高级命令

top

iotop

df -h

grep 

sed 

awk

netstat

halt

ps

find

## 1.2 查看进程、查看端口号、查看磁盘使用情况

top  ps netstat df -h

## 1.3 sed awk cut sort

只需要记住名称即可，最好总结一下这些命令的用法

## 1.4 写过哪些shell脚本

### 1.4.1 集群启停脚本

```bash
#! /bin/bash
case $1 in
"start"){
	for i in Hadoop02 Hadoop03 Hadoop04
	do
		ssh $i "绝对路径 start"
	done

};;
"stop"){

};;
```

### 1.4.2 数仓层级导入

ods=>dwd=>dws

```bash
#! /bin/bash

#定义变量
hive=/opt/module/hive/bin/hive
APP=gmall
#定义时间

sql = "

遇到表加上数据库名称
遇到时间，$do_data

"

hive -e "$sql"
```

### 1.4.3 数仓与MySQL的导入导出



## 1.5 单引号和双引号的区别

双引号可以取出里面变量的值，单引号不可以



# 2. Hadoop

## 2.1 入门

### 2.1.1 端口号

**Hadoop2系列的端口号:**

50070:查看HDFS运行情况

8088:查看任务运行情况

19888:历史服务器

9000:外部访问集群的一个通信端口



**Hadoop2系列的端口号:**

50070 -> 9870

9000 -> 8020

### 2.1.2 配置文件

**4个common**

core-site.xml

hdfs-site.xml

yarn-site.xml

mapred-site.xml

**3个env**

slaves(Hadoop3变更为workers) 该文件不能有空行、空格

## 2.2 HDFS

### 2.2.1 读写流程(笔试题)





### 2.2.2 小文件

+ 危害

  **NameNode的内存不够**

  128G内存能够存储的文件块数量为128\*1024\*1024 / 150字节 = 9亿

  **计算**

  一个文件对应一个切片，对应一个mapTask,容易资源耗尽

+ 解决

  har归档、自定义Inputformat > 减少NameNode内存

  使用CombineTextInputformat > 减少切片数 > 减少内存数

  JVM重用



### 2.2.3 块大小、副本数

128m Hadoop2默认

64m Hadoop1默认

32m 本地默认

256m hive默认 大厂

## 2.3 MapReduce(shuffle+优化)

Map方法之后，reduce方法之前

画图

MapReduce过程中哪些过程可以进行压缩



内存分配

![image-20210208110352576](/Users/cuiguangsong/go/src/docs/BigData/面试/images/1.png)

## 2.4 Yarn

### 2.4.1 工作机制



### 2.4.2 调度器

+ FIFO调度器

+ 容量调度器

+ 公平调度器

**默认调度器：**

Apache:容量调度器

cdh:公平调度器

每个调度器的特点

FIFO:单队列；先进先出，在企业里面不使用

容量调度器:支持多队列，跨队列借用资源，并发度较高，一般在中小企业中使用

公平调度器:支持多队列，跨队列借用资源，并发度最高，一般在中大型公司使用



**在企业中怎么配置任务队列**

容量调度器默认只有一个default

按照框架名称配置队列:hive/spark/flink/tez

按照业务名称配置队列:购物车、订单、注册



# 3. Zookeeper

## 3.1 安装台数

安装奇数台

10台服务器：3

20台服务器：5

50台服务器:7

100台服务器:11

台数:

​	过少：影响可靠性

​	过多：影响通信时间

## 3.2 选举机制

半数机制



## 3.3 常用命令

ls create get



# 4. flume

## 4.1 组成

+ source 

  TaildirSource好处:

  断点续传、多目录

  

  哪个版本产生的上述特点 Apache 1.7  CDH 1.6

  

  没出现之前taildir之前怎么做的：自定义source

  

  挂了怎么办？有可能重复数据、不会丢数

  

  如果产生了重复数据，怎么办？

  不处理:在企业里面很多情况就不处理，产生重复的情况本身概率比较低，如果为了防止概率比较低的事情发生而做了大量的事务，会严重影响性能

  处理:增加事务

  ​         在下一级处理: hive dwd层；SparkStreaming  redis

  ​          具体的处理手段:开窗取第一条或者分组

  

  taildir是否支持递归

  默认不支持，如果想要支持递归需要自定义代码(递归代码+读取文件代码)

  

  

+ channel

  file channel:基于磁盘，传输慢，可靠性高,默认存储容量是100万个event，配置多目录(多个磁盘)

  Memory channel：基于内存，传输快，可靠性差,默认存储容量100个event

  Kafka channel：数据存储在kafka,基于磁盘，性能优于memory channel+ kafka sink,原因是减少了sink阶段

     						Kafka channel是哪个版本产生的？ apache flume 1.6产生的,开始时没活起来，到了1.7才火起来，1.6的时候是 topic+内容，不要topic的那个参数设置不起作用，1.7才修复

​              在生产环境中该怎么选择channel?

​				下一级如果是kafka，优先选择kafka channel

​				如果公司比较看重可靠性(金融公司)，选择filechannel

​				如果公司比较看重效率(传输的是普通的日志) 选memory channel

+ sink

  Hdfs sink 小文件 

  小文件的解决方法：har归档、combine...(这个没太听清)、JVM重用

  文件大小(128m)、时间(1-2个小时)、event个数(禁用，大小不一，不方便处理)

+ 事务

  


## 4.2 3个器

### 4.2.1 拦截器

1. etl拦截器

   判断json是否完整，是否是{开头，}结尾

2. 分类型拦截器

   不分类型的话只有start、event两种类型

   (点赞、评论、收藏)

   商品点击、列表、详情

   故障

   广告

   前台活跃、后台活跃、通知

3. 自定义拦截器的步骤

   定义类，实现interceptor接口，重写里面4个方法：初始化、关闭、单event、多event处理

   还有一个静态内部类builder

   打包 --> 上传到指定目录flume/lib --> 跟配置文件进行关联 

4. 不要拦截器行不行

   可以

   在下一级处理：hive的dwd层、SparkStreaming

   如果对传输速度要求不高，就可以使用拦截器

   要求比较高，尤其是推荐，那就不能用拦截器

### 4.2.2 选择器

1. replicating选择器(默认)

   发往下一级所有channel

2. Multiplexing选择器

   选择性的发往指定的channel

### 4.2.3 监控器

ganglia

put/take尝试提交的次数远远大于最终成功的次数？如何优化

靠自己： 提高内存 flume_env.sh 调高内存 ：4-6G

找兄弟：增加flume台数，增加日志服务器（16G~64G都有，8T磁盘）

## 4.3 优化

### 4.3.1 file channel配置多目录



### 4.3.2 小文件问题的解决



### 4.3.3 监控器监控套到内存不足怎么办

靠自己： 提高内存 flume_env.sh 调高内存 ：4-6G

找兄弟：增加flume台数

### 4.3.4 flume挂了怎么办？

Taildir source有可能数据重复，如果是memory channel，有可能会丢100个event数据



# 5. kafka

## 5.1 基础

### 5.1.1 组成

#### 5.1.1.1 zookeeper里存储了哪些相关信息？

1. 没有存储任何producer相关信息
2. 存储了broker相关的信息(broker id)
3. 存储了consumer相关的信息(offset)
4. 存储了topic信息

#### 5.1.1.2 kafka部署多少台

2*n+1

其中n表示  生产者生产峰值的速率*副本数/100

#### 5.1.1.3 kafka 副本数多少个

2~3个，采用两个副本的居多

副本越多，可靠性越高

副本多了会增加磁盘IO,降低网络性能

#### 5.1.1.4 压测

测试kafka的峰值生产速率

测试kafka的峰值消费速率

峰值生产速率≤50M/s

#### 5.1.1.5 数据量

100万日活：每人一天多少条 100条 100万*100条=1亿

1条日志多大：0.5k~2k之间 一般取1k

Kafka平均每秒多少条日志：1亿/(24*3600)=1150条/s 大小的话是1m/s

什么时候达到高峰 8-12点 周末

高峰时是平时的20倍左右 20m/s 不要超过50m/s,超过的话需要增加kafka的台数

#### 5.1.1.6 kafka数据保存时间

默认保存7天，生产环境下保存3天

#### 5.1.1.7 kafka磁盘大小

100g * 副本2 * 3天 /0.7 

#### 5.1.1.8 监控

监控器：kafka eagle

自己写的

#### 5.1.1.9 有多少个topic

满足下一级所有消费者

方案1：一张表一个topic

方案2：

小程序 -> 小程序topic

公众号 -> 公众号topic         -> kafka -> SparkStreaming解析 -> kafka

​																->  hive dwd层解析

pc网站 -> pc topic

 #### 5.1.1.10 isr

leader挂了谁当老大？

在isr队列里的都有机会

怎么进入isr队列？

旧版本：延迟时间、延迟条数

新版本：延迟时间

#### 5.1.1.11 分区数

设置多少个分区比较合适？

设总的目标吞吐量为Tt,生产者的吞吐量为Tp,消费者的吞吐量为Tc，那么分区数：

分区数=Tt / min(Tp,Tc)

步骤：

先设置一个分区，然后压测，得出Tc和Tp

分区的作用是提高并发度

#### 5.1.1.12 分区分配策略

1. range(默认)

   比如说有10个分区，3个消费线程，除不尽的情况的话会把多的那个放在第一个分区

   1 2 3 4

   5 6 7

   8 9 10

2. RoundRobin

   所有分区采用hash随机打散，再采用轮询的方式进行消费

## 5.2 挂了

日志服务器 -> Flume -> kafka集群

短时间内flume channel可以抗一段时间

如果没抗住，日志服务器还保存30天的日志

## 5.3 丢数

是否丢数，主要看ack的值

ack有3个取值：0，1，-1

1. ack=0

   发送过来，没有应答，传输速度最快，可靠性最差，在生产环境几乎不用

2. ack=1

   发送过来，leader应答，传输速度较快，可靠性可以，生产

3. ack=-1

   发送过来，leader+follower应答,传输速度慢，可靠性最高

4. 在生产环境下怎么选择

   0 在生产环境下几乎不用

   1 如果是普通的日志，对可靠性要求不是很高，一般选择1 大多数企业选择1

   -1 一般是应用到金融行业

## 5.4 重复了

如何保证不重复？

幂等性 + 事务 + ack=-1

如果重复了该怎么去重

在Hive 的dwd层、SparkStreaming、redis去重

去重的手段：分组、开窗取第一条

幂等性:单分区、一个会话内

在单分区内部，有个id，有数据进来的话会比较一下id是否重复，如果重复就不写入了

在一个会话内，最担心的就是挂了，重新启动后会重新建立会话，id全部清空，就有可能导致数据的重复

事务

事务也会维护一个id，在写入之前会判断id是否重复，使用事务的话需要把每个分区都判断一遍，严重影响性能

## 5.5 积压了

需要考虑是什么原因导致的积压



解决方案：

1. 自身

   增加分区(间接增加并发度)，SparkStreaming的cpu核数也需要对应增加

2. 找兄弟

   增加消费者消费的条数 增加消费的速度

   Batchsize



## 5.6 优化

![image-20210228114229444](./images/2.png)

![image-20210228114314339](./images/3.png)

## 5.7 kafka高效读写数据

### 5.7.1 kafka本身是分布式集群，同时采用分区技术，并发度高



### 5.7.2 顺序写磁盘

![image-20210228114643452](./images/4.png)

### 5.7.3 零拷贝技术

linux的拷贝技术，不经过应用层，直接从底层进行拷贝



## 5.8 Kafka支持传输

![image-20210228114941411](./images/5.png)

## 5.9 kafka过期数据清理

![image-20210228115143067](./images/6.png)

## 5.10 kafka可以按照时间消费数据

![image-20210228115302348](./images/7.png)

## 5.11 Kafka消费者角度考虑是拉取数据还是推送数据

拉取数据

## 5.12 kafka中的数据是有序的吗

单分区有序

多分区、分区与分区之间无序



# 6. Hive

## 6.1 组成

![image-20210228120217557](./images/8.png)



## 6.2 与MySQL的区别

​						Hive								MySQL

数据量      		大										小

速度        大数据量场景快					小数据量快

## 6.3 内部表与外部表的区别

元数据和原始数据

删除内部表：删除元数据和原始数据

删除外部表：只删除元数据

在生产环境下，什么时候创建外部表，什么时候创建内部表？

绝大多数场景都是创建外部表

自己创建的临时表会是内部表

## 6.4 四个by

### 6.4.1 order by

全局排序

在企业中用的并不多，原因是容易导致数据倾斜



### 6.4.2 sort by

分区内部排序

### 6.4.3 distributed by

分区

### 6.4.4 cluster by

sort + distributed by 字段相同

在企业中用的比较多的是:分区+排序



## 6.5 系统函数

### 6.5.1 日

date_add

date_sub



### 6.5.2 周

next_day()

### 6.5.3 月

last_day

date_format



### 6.5.4 json

get_json_object()

## 6.6 自定义函数

### 6.6.1 UDF

解析公共字段

### 6.6.2 UDAF



### 6.6.3 UDTF

解决事件字段

### 6.6.4 不用自定义函数行不行

行

为什么还要用？

方便debug调试

处理问题更灵活一些

主要用来解决复杂的业务逻辑

### 6.6.5 自定义UDF的步骤

定义类，继承UDF,重写里面evaluate方法

打包->上传到集群路径->在hive客户端注册

### 6.6.6 自定义UDTF步骤

定义类继承GenericUDTF,重写里面3个方法：初始化(定义了输出的名称和返回类型)、关闭、process

## 6.7 窗口函数

rank

over

**需要会手写TopN!!!**

## 6.8 优化

### 6.8.1 mapjoin 默认打开，不要关闭



### 6.8.2 行列过滤

SQL优化面试题： join where  ->  where join

### 6.8.3 采用分区

避免后续的全表扫描

### 6.8.4 列式存储(加快查询速度)



### 6.8.5 压缩(减少磁盘IO)



### 6.8.6 合理设置map个数

max(1,min(块大小，Long的最大值))

128M数据 对应一个MapTask



### 6.8.7 合理设置reduce个数

可以根据数据量适当增加reduce个数



### 6.8.8 小文件是如何产生的

1. 动态分区
2. reduce个数设置的过多  分区的key设计的不合理
3. 数据本身就是小文件

### 6.8.9 解决小文件问题

1. CombineHiveInputformat

2. 开启JVM重用

   set mapreduce.job.jvm.numtasks=10

3. merge 如果是maponly任务，默认功能打开

   执行完任务，产生小文件，再开启一个作业，这个作业负责将小于16M的文件进行合并，默认合格并到256M

   如果是MR任务，merge功能需要手动打开

### 6.8.10 在map阶段开启combiner

set hive.map.aggr=true

### 6.8.11 替换引擎

1. MR引擎：基于磁盘  						速度慢，  跨天、数据量比较大的任务
2. Tez引擎：基于内存                          速度快      数据量比较小，临时测试，验证
3. Spark引擎：基于磁盘+内存            速度中     当天的定时任务

## 6.9 数据倾斜

### 6.9.1 数据倾斜张什么样

![image-20210228124647846](/Users/cuiguangsong/go/src/docs/BigData/面试/images/9.png)

其他任务全部结束，只有某一个或几个特殊任务一直结束不了

### 6.9.2 怎么产生的数据倾斜

![image-20210228124849320](./images/10.png)

不同表的相同字段类型不一致时进行join会导致数据倾斜

解决方案：将其中一个表的字段的类型进行转换，使两者一致

![image-20210228125128966](./images/11.png)

### 6.9.3 解决数据倾斜的方法

![image-20210228125331492](./images/12.png)

![image-20210228125428185](./images/13.png)

![image-20210228125953245](./images/14.png)



## 6.10 Hive里边字段的分隔符

问题：Hive里字段分隔符用的什么？为什么用\t?有遇到过字段里面有\t的情况吗？怎么处理的？

![image-20210228130300019](/Users/cuiguangsong/go/src/docs/BigData/面试/images/15.png)



还有就是通过代码规范提前约定好分割符

## 6.11 Tez引擎的优点 

![image-20210228130542411](./images/16.png)

## 6.12 MySQL

![image-20210228130625002](./images/17.png)

![image-20210228130818190](./images/18.png)

![image-20210228130919215](./images/19.png)

![image-20210228130947047](./images/20.png)

## 6.13 Union和Union all区别

![image-20210228131035921](./images/21.png)

# 7. Sqoop

## 7.1 在使用Sqoop过程中遇到过哪些问题，怎么解决的？

### 7.1.1 \N,null

hive底层存储空值是\N，MySQL底层存储空值是null

### 7.1.2 临时表

-staging-table

## 7.2 参数

![image-20210228144852370](./images/22.png)

## 7.3 Sqoop底层运行的任务是什么

只有Map阶段，没有Reduce阶段的任务。默认是4个MapTask

## 7.4 Sqoop一天导入多少数据

100万日活 ->10万订单，1人10条，每天1G左右业务数据

Sqoop每天将1G的数据量导入数仓

## 7.5 Sqoop数据导出的时候一次执行多长时间

每天晚上00:30开始执行，Sqoop任务一般情况40~50分钟的都有

双11，618等活动在1个小时左右

如果没有用户行为数据的话可以从零点就开始导

原因是kafka中的数据不一定消费完

为什么00:30的时候kafka中的数据就消费完了呢？

kafka峰值消费速度为50m/s

50/s * 30  * 60 =90G



## 7.6 Sqoop在导入数据的时候发生数据倾斜

![image-20210228150456656](./images/24.png)

![image-20210228150644839](./images/25.png)

## 7.7 Sqoop数据导出Parquet(项目中遇到的问题)

![image-20210228150842949](./images/26.png)

## 7.8 Sqoop导出数据一致性问题

![image-20210228151120230](./images/27.png)

# 8. Azkaban

## 8.1 每天集群运行多少个指标

![image-20210228151605482](./images/28.png)

## 8.2 任务挂了怎么办

发邮件

发钉钉、企业微信

打电话

自定义报警基于onealert

# 9. 数仓架构

![image-20210228153329655](./images/29.png)

![image-20210228153639583](./images/30.png)

## 9.1 数仓概念

![image-20210228153844027](./images/31.png)

数仓分层的几个单词缩写

![image-20210228162303688](./images/38.png)

## 9.2 框架版本选型

![image-20210228154421236](./images/32.png)

目前HDP已经被CDH收购，目前两者统一称为CDP,目前版本是7.0

CDH最高的免费版本是6.3.2

![image-20210228155457206](./images/33.png)

![image-20210228155521318](./images/34.png)

## 9.3 服务器选型

![image-20210228155623770](./images/35.png)

![image-20210228160501193](./images/36.png)

## 9.4 集群规模

![image-20210228161101860](./images/37.png)

## 9.5 集群架构

![image-20210228163123596](./images/39.png)

![image-20210228163542724](./images/40.png)

![image-20210228163722918](./images/41.png)

![image-20210228164004032](./images/42.png)

## 9.6 人员配置

![image-20210228164750288](./images/43.png)

# 10. 数仓项目

## 10.1 ODS(Operation Data Store)

1. 保持数据原貌，不做任何修改
2. 压缩 LZO,目的：减少磁盘空间
3. 创建分区，目的：减少后续的全表扫描

## 10.2 DWD(Data Warehouse Detail)

1. ETL 清洗

   清洗掉哪些数据

   根据业务将字符代表的含义转换成对应的业务

   核心字段不能为空，为空就清洗掉

   超时的数据过滤掉

   重复的数据过滤掉

2. 清洗的工具

   hive sql

   spark sql

   python

   mapreduce

   Kettle

3. 清洗掉多少脏数据后才认为数据是ok的？

   万分之一

4. 维度退化

   商品三级分类、二级分类、一级分类、商品表 退化成商品表

   省、市、县 退化成 地区表

   时间(年、月、日) 退化成 时间表

5. 为什么要退化

   防止后续的频繁join

6. 压缩

   减少磁盘空间

7. 列式存储

   采用Parquet，目的是提高查询速度

8. 创建分区表

9. 脱敏

   个人信息、身份证、手机号

## 10.3 DWS(Data Warehouse Service)

DWS层有3-5张宽表(处理100-200个指标 70%以上的需求)

设备、会员、商品、购买、活动、优惠券

哪个宽表最宽

会员宽表：60个~100个

站在维度的角度看事实，看事实表的度量值(个数、件数、次数、金额)

## 10.4 DWT(Data )



## 10.5 ADS(Application Data Store)

### 10.5.1 分析过哪些指标

![image-20210228203456887](./images/44.png)

![image-20210228203730545](./images/45.png)

### 10.5.2 ADS层手写指标

1. 如何分析用户活跃

   在启动日志中统计不同设备id出现次数

2. 如何分析用户新增

   用户活跃表left join用户新增表，用户新增表中mid为空的即为用户新增

3. 如何分析用户1天留存

   留存用户=前一天新增 join 今天活跃

   用户留存率=留存用户/前一天新增

4. 如何分析沉默用户

   登录时间为7天前，且只出现过一次

   按照设备id对日活表分组，登录次数为1，且是在一周前登录

5. 如何分析本周回流用户

   本周活跃left join 本周新增 left join 上周活跃，且本周新增id和上周活跃id都为?

### 10.5.3 流转G复活指标









# 99. 项目经验

## 99.1 伊对离线分析平台

工作流程

1.Flume配置Kafka Source，将数据从Kafka中保存到HDFS上

2.使用Shell脚本周期性的将HDFS上的数据导入到Hive中

3.HDFS中的日志数据直接导入到ODS层，在ODS层保持原有数据不变，使用Sqoop将MySQL中的业务数据导入到ODS层，并且使用Lzo+Parquet压缩和存储

4.对ODS中数据进行清洗、脱敏和降维后，编写Shell脚本导入到DWD层，并且使用Lzo+Parquet压缩和存储

5.根据DWD层中的数据建立用户行为宽表、商品宽表、订单宽表，对业务变化缓慢的表建立拉链表，导入DWS层

6.在DWS层统计各个主题对象的当天行为，在DWT层以分析的主题对象为建模驱动，基于上层的应用和产品的指标需求，构建主题对象的全量宽表。

7.在Hive中配置Tez引擎，将多个有依赖的作业转换为一个作业，提升计算性能

8.使用SparkSQL分析指标，并将统计分析后的数据导入到ADS层

9.编写各层数据的导入和解析脚本，使用Azkaban进行全任务调度

10.使用Kylin来完成一些即席查询

11.使用Superset来进行数据可视化展示

12.使用Atlas进行元数据的管理，Griffin进行数据质量监控，Senrty进行权限管理，Zabbix进行集群的监控。

13.根据Hive的使用情况对其进行相应的优化操作，比如小文件处理，map、reduce的配置等

14.Hive解决数据倾斜问题，如mapjoin，开启数据倾斜时的负载均衡等

15.Hive合理采用分区技术






















