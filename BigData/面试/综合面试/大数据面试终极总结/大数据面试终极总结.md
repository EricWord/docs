[TOC]

# 1. 开发语言方面



## 1.1 Go

### 1.1.1 Channel

1. CSP模型(面试常常问)

2. 创建

   ```go
   ch := make(chan int)
   ```

   channel和map类似，make创建了一个底层数据结构的引用，当赋值或参数传递时，只是拷贝了一个channel引用，指向相同的chnnel对象。和其他引用类型一样，channel的空值为nil,使用==可以对类型相同的channel进行比较，只有指向相同对象或者同为Nil时，才返回true

3. 读写操作

   ```go
   ch := make(chan int)
   
   // write to channel
   ch <- x
   
   // read from channel
   x <- ch
   
   // another way to read
   x = <- ch
   ```

   Channel一定要初始化后才能进行读写操作，否则会永久阻塞

4. 关闭(面试常问，细节需要注意)

   ```go
   ch := make(chan int)
   close(ch)
   ```

   下面6点一定要熟记:

   + 关闭一个未初始化/nil的channel会panic
   + 重复关闭同一个channel会panic
   + 向一个已经关闭的channel发送消息会panic
   + 从已关闭的channel读取消息不会产生panic,且能读出channel中还未被读取的消息，若消息均已经读出，会读到类型的零值。从一个已关闭的channel中读取消息永远不会阻塞，并且会返回一个为false的ok-idiom，可以用它来判断channel是否关闭。
   + 关闭channel会产生一个广播机制，所有从channel读取消息的goroutine都会收到消息
   + 向nil的channel里写会panic
   + 从一个为Nil的channel里读会panic

   **注意channel如果需要关闭的话一定是写的那端来关闭channel**

5. 带缓存的和不带缓存的

   + 无缓存的channel

     从无缓存的channel中读取消息会阻塞，直到有goroutine向该channel中发送消息

     向无缓存的channel中发送消息也会阻塞，直到有goroutine从channel中读取消息

   + 有缓存的channel

     有缓存的channel的声明方式为指定make函数的第二个参数，该参数为channel缓存的容量

     ```go
     ch := make(chan int, 10)
     ```

     有缓存的channel类似于一个阻塞队列(采用环形数组实现)。当缓存未满时，向channel中发送消息不会阻塞，当缓存满时，发送操作将被阻塞，直到有其他goroutine从中读取消息；相应的，当channel中消息不为空时，读取消息不会出现阻塞，当channel为空时，读取操作会造成阻塞，直到有goroutine向channel中写入消息。

     通过len函数可以获得chan中的元素个数，通过cap函数可以得到channel的缓存长度

   

6. 用法

   + goroutine之间通信

     ```go
     c := make(chan int)  // Allocate a channel.
     
     // Start the sort in a goroutine; when it completes, signal on the channel.
     go func() {
         list.Sort()
         c <- 1  // Send a signal; value does not matter.
     }()
     
     doSomethingForAWhile()
     <-c
     ```

     主goroutine会阻塞，直到执行sort的goroutine完成

   + range遍历

     Channel也可以使用range取值，并且会一直从channel中读取数据，直到有goroutine对该channel执行close操作，循环才会结束

     ```go
     // consumer worker
     ch := make(chan int, 10)
     for x := range ch{
         fmt.Println(x)
     }
     ```

     在for执行的过程中，如果channel已经close，for循环会自动退出

     上面的代码等价于

     ```go
     for {
         x, ok := <- ch
         if !ok {
             break
         }
         
         fmt.Println(x)
     }
     ```

   + 配合select使用

     Select用法类似于IO多路复用，可以同时监听多个channel的消息状态

     ```go
     select {
         case <- ch1:
         ...
         case <- ch2:
         ...
         case ch3 <- 10;
         ...
         default:
         ...
     }
     ```

     + Select可以同时监听多个channel的写入和读取
     + 执行select时，若只有一个case通过(不阻塞)，则执行这个case块
     + 若有多个case通过，则随机挑选一个case执行
     + 若所有case均阻塞，且定义了default模块，则执行default模块。若未定义default模块，则select语句阻塞，直到有case被唤醒
     + 使用break会跳出select块
     + 可以将某个channel设置为nil，进行强制阻塞，对于select分支来说，就是强制禁用此分支

     **设置超时时间**

     ```go
     ch := make(chan struct{})
     
     // finish task while send msg to ch
     go doTask(ch)
     
     timeout := time.After(5 * time.Second)
     select {
         case <- ch:
             fmt.Println("task finished.")
         case <- timeout:
             fmt.Println("task timeout.")
     }
     ```

     **Quit channel**

     有一些场景中，一些worker goroutine需要处理一直循环处理信息，直到收到quit信号

     ```go
     msgCh := make(chan struct{})
     quitCh := make(chan struct{})
     for {
         select {
         case <- msgCh:
             doWork()
         case <- quitCh:
             finish()
             return
     }
     ```

     **单向channel**

     所谓单向channel就是只可写入或只可读的channel,事实上channel只读或只写都没有意义，所谓的单向channel其实只是声明时使用

     ```go
     func foo(ch chan<- int) <-chan int {...}
     ```

     `chan <- int`表示一个只可以写入的channel,`<-chan int`表示一个只可以读取的channel。上面这个函数约定了`foo`内只能向ch中写入数据，返回一个只能读取的channel,虽然使用普通的channel也没有问题，但这样在方法声明时约定可以防止channel被滥用，这种预防机制发生在编译期。

   + 合并多个channel的输出到一个channel形成一个消息队列

     ```go
     /**
     将多个输入的channel进行合并成一个channel
      */
     func testMergeInput() {
     	input1 := make(chan int)
     	input2 := make(chan int)
     	output := make(chan int)
     
     	go func(in1, in2 <-chan int, out chan<- int) {
     		for {
     			select {
     			case v := <-in1:
     				out <- v
     			case v := <-in2:
     				out <- v
     			}
     		}
     	}(input1, input2, output)
     
     	go func() {
     		for i := 0; i < 10; i++ {
     			input1 <- i
     			time.Sleep(time.Millisecond * 100)
     		}
     	}()
     
     	go func() {
     		for i := 20; i < 30; i++ {
     			input2 <- i
     			time.Sleep(time.Millisecond * 100)
     		}
     	}()
     
     	go func() {
     		for {
     			select {
     			case value := <-output:
     				fmt.Println("输出：", value)
     			}
     		}
     	}()
     
     	time.Sleep(time.Second * 5)
     	fmt.Println("主线程退出")
     }
     ```

     

7. channel的组成

   主要组成:一个环形数组实现的队列，用于存储消息元素；两个链表实现的goroutine等待队列，用于存储阻塞在recv和send操作上的goroutine;一个互斥锁，用户各个属性变动的同步

8. 通过channel传递的数据

   经过channel传递的数据是相互独立的，修改传递之后的数据不会影响修改之前的数据

9. 生产者消费者问题

   通过channel可以比较方便的实现生产者消费者模型，这里开启一个生产者线程，一个消费者线程，生产者线程往channel中发送消息，同时阻塞，消费者线程轮询获取channel中的消息，进行处理，然后阻塞，这时生产者线程唤醒继续后面的逻辑，如此便形成了简单的生产者消费者模型。同时生产者在完成了所有的消息发送后，可以通过quit这个channel通知消费者线程退出，而消费者线程退出时，通知主线程退出，整个程序完成退出。

   ```go
   /**
   生产者消费者问题
    */
   func testPCB() {
   	fmt.Println("test PCB")
   
   	intchan := make(chan int)
   	quitChan := make(chan bool)
   	quitChan2 := make(chan bool)
   
   	value := 0
   
   	go func() {
   		for i := 0; i < 3; i++ {
   
   			value = value + 1
   			intchan <- value
   
   			fmt.Println("write finish, value ", value)
   
   			time.Sleep(time.Second)
   		}
   		quitChan <- true
   	}()
   	go func() {
   		for {
   			select {
   			case v := <-intchan:
   				fmt.Println("read finish, value ", v)
   			case <-quitChan:
   				quitChan2 <- true
   				return
   			}
   		}
   
   	}()
   
   	<-quitChan2
   	fmt.Println("task is done ")
   }
   ```

10. 通过channel实现并发数量控制

    通过设置一个带缓冲数量的的channel来实现最大并发数量，最大并发数量即为缓冲数量，任务开始时想limit这个channel发送消息，任务执行完成后从这个limit读取消息，这样就可以保证当并发数量达到limit的缓冲数量时，limit <- true 这里会发生阻塞，停止创建新的线程，知道某个线程执行完成任务后，从limit读取数据，这样就能保证最大并发数量控制在缓冲数量。

    ```go
    /*
    测试通过channel来控制最大并发数，来处理事件
     */
    func testMaxNumControl()  {
    	maxNum := 3
    	limit := make(chan bool, maxNum)
    	quit := make(chan bool)
    
    	for i:=0; i<100; i++{
    		fmt.Println("start worker : ", i)
    
    		limit <- true
    
    		go func(i int) {
    			fmt.Println("do worker start: ", i)
    			time.Sleep(time.Millisecond * 20)
    			fmt.Println("do worker finish: ", i)
    
    			<- limit
    
    			if i == 99{
    				fmt.Println("完成任务")
    				quit <- true
    			}
    
    		}(i)
    	}
    
    	<-quit
    	fmt.Println("收到退出通知，主程序退出")
    }
    ```

    

11. 监听中断信号的channel

    ```go
    quit := make(chan os.Signal)
    signal.Notify(quit, os.Interrupt)
    <- quit
    ```

    

12. 利用channel实现同步控制

    开启多个线程做赚钱和花钱的操作，共享读写remainMoney这个剩余金额变量，实现生产者消费者模型

    ```go
    //同步控制模型，生产者模型
    var lockChan = make(chan int, 1)
    var remainMoney = 1000
    func testSynchronize()  {
    	quit := make(chan bool, 2)
    
    	go func() {
    		for i:=0; i<10; i++{
    			money := (rand.Intn(12) + 1) * 100
    			go testSynchronize_expense(money)
    
    			time.Sleep(time.Millisecond * time.Duration(rand.Intn(500)))
    		}
    
    		quit <- true
    	}()
    
    	go func() {
    		for i:=0; i<10; i++{
    			money := (rand.Intn(12) + 1) * 100
    			go testSynchronize_gain(money)
    
    			time.Sleep(time.Millisecond * time.Duration(rand.Intn(500)))
    		}
    
    		quit <- true
    	}()
    
    	<- quit
    	<- quit
    
    	fmt.Println("主程序退出")
    }
    
    func testSynchronize_expense(money int)  {
    	lockChan <- 0
    
    	if(remainMoney >= money){
    		srcRemainMoney := remainMoney
    		remainMoney -= money
    		fmt.Printf("原来有%d, 花了%d，剩余%d\n", srcRemainMoney, money, remainMoney)
    	}else{
    		fmt.Printf("想消费%d钱不够了, 只剩%d\n", money, remainMoney)
    	}
    
    	<- lockChan
    }
    
    func testSynchronize_gain(money int)  {
    	lockChan <- 0
    
    	srcRemainMoney := remainMoney
    	remainMoney += money
    	fmt.Printf("原来有%d, 赚了%d，剩余%d\n", srcRemainMoney, money, remainMoney)
    
    	<- lockChan
    }
    ```

13. channel是否关闭的判断

    读已关闭的channel会得到零值，如果不确定channel是否关闭，需要使用`ok`进行检测。ok的结果和含义：

    - `true`：读到数据，并且通道没有关闭。
    - `false`：通道关闭，无数据读到。

    ```go
    if v, ok := <- ch; ok {
        fmt.Println(v)
    }
    ```

### 1.1.2 defer

1. defer的用途

   + 关闭文件句柄
   + 锁资源释放
   + 数据库连接释放

2. 输出顺序

   大原则是先进后出

   ```go
   package main
   
   import "fmt"
   
   func main() {
       var users [5]struct{}
       for i := range users {
           defer fmt.Println(i)
       }
   }
   ```

   输出：4 3 2 1 0

   如果上面的代码在defer中使用闭包，那么打印结果就不一样le

   ```go
   package main
   
   import "fmt"
   
   func main() {
       var users [5]struct{}
       for i := range users {
           defer func() { fmt.Println(i) }()
       }
   }
   ```

   输出：4 4 4 4 4

   原因：此处变量i在循环执行的时候是会被复用的，最后一次for循环i被赋值为4，所以最终输出的结果就是4

   不使用闭包的话，而是改用函数就可以正常输出预期的 4 3 2 1 0

   ```go
   package main
   
   import "fmt"
   
   func main() {
       var users [5]struct{}
       for i := range users {
           defer Print(i)
       }
   }
   func Print(i int) {
       fmt.Println(i)
   }
   ```

   

3. defer调用引用结构体函数

   ```go
   package main
   
   import "fmt"
   
   type Users struct {
       name string
   }
   
   func (t *Users) GetName() { // 注意这里是 * 传地址 引用Users
       fmt.Println(t.name)
   }
   func main() {
       list := []Users{{"乔峰"}, {"慕容复"}, {"清风扬"}}
       for _, t := range list {
           defer t.GetName()
       }
   }
   ```

   输出：清风扬 清风扬 清风扬

   如果向打印出 清风扬 慕容复 乔峰 需要使用类似下面的写法

   ```go
   package main
   
   import "fmt"
   
   type Users struct {
       name string
   }
   
   func (t *Users) GetName() { // 注意这里是 * 传地址 引用Users
       fmt.Println(t.name)
   }
   func GetName(t Users) { // 定义一个函数，名称自定义
       t.GetName() // 调用结构体USers的方法GetName
   }
   func main() {
       list := []Users{{"乔峰"}, {"慕容复"}, {"清风扬"}}
       for _, t := range list {
           defer GetName(t)
       }
   }
   ```

   输出：清风扬 慕容复 乔峰

   修改一下最初的代码

   ```go
   package main
   
   import "fmt"
   
   type Users struct {
       name string
   }
   
   func (t *Users) GetName() { // 注意这里是 * 传地址 引用Users
       fmt.Println(t.name)
   }
   func GetName(t Users) { // 定义一个函数，名称自定义
       t.GetName() // 调用结构体USers的方法GetName
   }
   func main() {
       list := []Users{{"乔峰"}, {"慕容复"}, {"清风扬"}}
       for _, t := range list {
           t2 := t // 定义新变量t2 t赋值给t2
           defer t2.GetName()
       }
   }
   ```

   输出：清风扬 慕容复 乔峰

4. 多个defer的执行顺序

   多个defer注册，按照先进后出的次序执行，哪怕函数或者某个延迟调用出错，这些调用依旧会被执行

   ```go
   package main
   
   func users(i int) {
       defer println("北丐")
       defer println("南帝")
   
       defer func() {
           println("西毒")
           println(10 / i) // 异常未被捕获，逐步往外传递，最终终止进程。
       }()
   
       defer println("东邪")
   }
   
   func main() {
       users(0)
       println("武林排行榜,这里不会被输出哦")
   }
   ```

   输出:

   东邪
   西毒
   南帝
   北丐
   panic: runtime error: integer divide by zero
   goroutine 1 [running]:
   main.users.func1(0x0)

   

5. 延迟调用参数在求值或复制，指针或闭包会"延迟"读取

   ```go
   package main
   
   func test() {
       x, y := "乔峰", "慕容复"
   
       defer func(s string) {
           println("defer:", s, y) // y 闭包引用 输出延迟和的值，即y+= 后的值=慕容复第二
       }(x) // 匿名函数调用，传送参数x 被复制,注意这里的x 是 乔峰,而不是下面的 x+= 后的值
   
       x += "第一"
       y += "第二"
       println("x =", x, "y =", y)
   }
   
   func main() {
       test()
   }
   ```

   输出：

   x = 乔峰第一 y = 慕容复第二
   defer: 乔峰 慕容复第二

   但是如果向下面这样的代码输出就会发生改变:

   ```go
   package main
   
   func test() {
   	x, y := "乔峰", "慕容复"
   	defer println("defer:", y)
   	x += "第一"
   	y += "第二"
   	println("x =", x, "y =", y)
   }
   
   func main() {
   	test()
   }
   ```

   x = 乔峰第一 y = 慕容复第二
   defer: 慕容复

   

6. defer在匿名返回值和命名返回值函数中的表现不同

   ```go
   package main
   
   import "fmt"
   
   func Users() (s string) {
   
   	s = "乔峰"
   	defer func() {
   		fmt.Println("延迟执行后:" + s)
   	}()
   
   	return "清风扬"
   }
   
   func main() {
   	println(Users())
   }
   ```

   输出：

   延迟执行后:清风扬
   清风扬

   通过上面这段代码可以总结出return和defer的执行顺序如下:

   先执行return语句的赋值，然后执行defer语句，最后执行return

   也就是说return不是原子操作

   

   再看一个例子

   ```go
   func returnValues() int {
       var result int
       defer func() {
           result++
           fmt.Println("defer")
       }()
       return result
   }
   
   func namedReturnValues() (result int) {
       defer func() {
           result++
           fmt.Println("defer")
       }()
       return result
   }
   ```

   上面的方法会输出0，下面的方法输出1。上面的方法使用了匿名返回值，下面的使用了命名返回值，除此之外其他的逻辑均相同，为什么输出的结果会有区别呢？

   要搞清这个问题首先需要了解defer的执行逻辑，文档中说defer语句在方法返回“时”触发，也就是说return和defer是“同时”执行的。以匿名返回值方法举例，过程如下。

   - 将result赋值给返回值（可以理解成Go自动创建了一个返回值retValue，相当于执行retValue = result）
   - 然后检查是否有defer，如果有则执行
   - 返回刚才创建的返回值（retValue）

   在这种情况下，defer中的修改是对result执行的，而不是retValue，所以defer返回的依然是retValue。在命名返回值方法中，由于返回值在方法定义时已经被定义，所以没有创建retValue的过程，result就是retValue，defer对于result的修改也会被直接返回。

   

7. 在for循环中使用defer可能导致性能问题

   ```go
   func deferInLoops() {
       for i := 0; i < 100; i++ {
           f, _ := os.Open("/etc/hosts")
           defer f.Close()
       }
   }
   ```

   defer在紧邻创建资源的语句后生命力，看上去逻辑没有什么问题。但是和直接调用相比，defer的执行存在着额外的开销，例如defer会对其后需要的参数进行内存拷贝，还需要对defer结构进行压栈出栈操作。所以在循环中定义defer可能导致大量的资源开销，在本例中，可以将f.Close()语句前的defer去掉，来减少大量defer导致的额外资源消耗。

8. 调用os.Exit时，defer不会被执行

   当发生panic时，所在goroutine的所有defer会被执行，但是当调用os.Exit()方法退出程序时,defer不会被执行

### 1.1.3 recover

recover仅在延迟函数defer中有效，在正常的执行过程中，调用recover会返回nil并且没有其他任何效果，如果当前的goroutine panic,调用recover可以捕获到panic的输入值，并且恢复正常的执行



## 1.2 Java

### 1.2.1 HashMap

重点掌握源码、预留数、扩容、对性能的影响

**读写的时间复杂度**

回答面试官的答案：

理想状态下读的时间复杂度是O(1)

读的理想情况：由于hashmap是通过数组+链表实现的，所以在链表长度尽可能短(为0)读的时间复杂度就是O(1)

理想情况下写的时间复杂度也是O(1)

同理，写的时间复杂度有可能是O(1)、O(logn)、O(n)

写(put)操作的流程：

第一步：key.hashcode()，时间复杂度O(1)。

第二步：找到桶以后，判断桶里是否有元素，如果没有，直接new一个entey节点插入到数组中。时间复杂度O(1)。

第三步：如果桶里有元素，并且元素个数小于6，则调用equals方法，比较是否存在相同名字的key，不存在则new一个entry插入都链表尾部。时间复杂度O(1)+O(n)=O(n)。

第四步：如果桶里有元素，并且元素个数大于6，则调用equals方法，比较是否存在相同名字的key，不存在则new一个entry插入都链表尾部。时间复杂度O(1)+O(logn)=O(logn)。红黑树查询的时间复杂度是logn。





和b+树的区别





1. 底层实现

   HashMap是由数组+链表组成的，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的

2. 扩容

   默认的负载因子大小为0.75,也就是说当一个map填满了75%的bucket的时候，和其他集合类一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中，这个过程叫做rehashing,因为它调用hash方法找到新的bucket位置，这个值只可能在两个地方，一个是原下标的位置，另一种是下标为<原下标+原容量>的位置

   

   另外一种：

   HashMap的初始桶的数量为16，loadFact为0.75,当桶里面的数据记录超过阈值的时候，HashMap将会进行扩容则操作，每次都会变为原来大小的2倍，直到设定的最大值之后就无法再resize了。

3. 为什么说HashMap是线程不安全的？

   或者换种问法，HashMap为什么在多线程的情况下会出现死循环

   + put的时候导致的多线程数据不一致

     有两个线程A和B，首先A希望插入一个key-value对到HashMap中，首先计算记录所要落到的桶的索引坐标，然后获取到该桶里面的链表头结点，此时线程A的时间片用完了，而此时线程B被调度得以执行，和线程A一样执行，只不过线程B成功将记录插到了桶里面，假设线程A插入的记录计算出来的桶索引和线程B要插入的记录计算出来的桶索引是一样的，那么当线程B成功插入之后，线程A再次被调度运行时，它依然持有过期的链表头但是它对此一无所知，以至于它认为它应该这样做，如此一来就覆盖了线程B插入的记录，这样线程B插入的记录就凭空消失了，造成了数据不一致的行为。

     

     

     

     ---

     上面的简化回答:

     在hashmap做put操作的时候会调用到`addEntry`方法。现在假如A线程和B线程同时对同一个数组位置调用addEntry，两个线程会同时得到现在的头结点，然后A写入新的头结点之后，B也写入新的头结点，那B的写入操作就会覆盖A的写入操作造成A的写入操作丢失

   + resize操作时的线程不安全

     addEntry中当加入新的键值对后键值对总数量超过门限值的时候会调用一个resize操作， 这个操作会新生成一个新的容量的数组，然后对原数组的所有键值对重新进行计算和写入新的数组，之后指向新生成的数组。当多个线程同时检测到总数量超过阈值的时候就会同时进行resize操作，各自生成新的数组并rehash后赋给该map底层的数组table,结果最终只有最后一个线程生成的新数组被赋给table变量，其他线程的均会丢失。而且当某些线程已经完成赋值而其他线程刚开始的时候，就会用已经被赋值的table作为原始数组，这样也会有问题

   

   

   + HashMap的get操作可能因为resize而引起死循环（cpu100%）

     下面是resize的代码

     ```java
     void transfer(Entry[] newTable, boolean rehash) {  
             int newCapacity = newTable.length;  
             for (Entry<K,V> e : table) {  
       
                 while(null != e) {  
                     Entry<K,V> next = e.next;           
                     if (rehash) {  
                         e.hash = null == e.key ? 0 : hash(e.key);  
                     }  
                     int i = indexFor(e.hash, newCapacity);   
                     e.next = newTable[i];  
                     newTable[i] = e;  
                     e = next;  
                 } 
             }  
         }  
     ```

     这个方法的功能是将原来的记录重新计算在新桶的位置，然后迁移过去

     ![image-20210407155326387](images/image-20210407155326387.png)

     我们假设有两个线程同时需要执行resize操作，我们原来的桶数量为2，记录数为3，需要resize桶到4，原来的记录分别为：[3,A],[7,B],[5,C]，在原来的map里面，我们发现这三个entry都落到了第二个桶里面。
      假设线程thread1执行到了transfer方法的Entry next = e.next这一句，然后时间片用完了，此时的e = [3,A], next = [7,B]。线程thread2被调度执行并且顺利完成了resize操作，需要注意的是，此时的[7,B]的next为[3,A]。此时线程thread1重新被调度运行，此时的thread1持有的引用是已经被thread2 resize之后的结果。线程thread1首先将[3,A]迁移到新的数组上，然后再处理[7,B]，而[7,B]被链接到了[3,A]的后面，处理完[7,B]之后，就需要处理[7,B]的next了啊，而通过thread2的resize之后，[7,B]的next变为了[3,A]，此时，[3,A]和[7,B]形成了环形链表，在get的时候，如果get的key的桶索引和[3,A]和[7,B]一样，那么就会陷入死循环。

     如果在取链表的时候从头开始取（现在是从尾部开始取）的话，则可以保证节点之间的顺序，那样就不存在这样的问题了。

   

4. HashMap和HashTable的区别

   + HashMap允许null key和null value,而HashTable不允许
   + HashMap不是线程安全的，HashTable是线程安全的
   + HashMap是非synchronized的，HashTable是synchronized的
   + HashMap的迭代器是fail-fast迭代器，而HashTable的迭代器不是fail-fast的，当有其他线程改变了HashMap的结构(增加或者移除元素)将会抛出ConcurrentModificationException
   + 由于Hashtable是线程安全的也是synchronized，所以在单线程环境下它比HashMap要慢
   + 继承的父类不同， Hashtable继承自Dictionary类，而HashMap继承自AbstractMap类。但二者都实现了Map接口
   + 

   

   

   

   

   

   

   

   

   
   

5. 解决hash冲突的方法

   

   

   

   

   

   

   

   

6. java7和java8 HashMap的变化

   ![image-20210407151021167](images/image-20210407151021167.png)

   变动/优化主要在上图中红框内的部分，在插入的时候，如果链表长度大于8，会将链表转换成红黑树进行插入

7. 如何让HashMap同步？

   ```java
   Map m = Collections.synchronizeMap(hashMap);
   ```

   

8. xxx





### 1.2.2 多线程模型实现





### 1.2.3 线程池



问：

java官方提供了几种线程池

各自的作用？



注意下面的线程池的拒绝策略面试会问到



1. 线程池使用实例代码

```java
public class Test {
     public static void main(String[] args) {   
         ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 200, TimeUnit.MILLISECONDS,
                 new ArrayBlockingQueue<Runnable>(5));
          
         for(int i=0;i<15;i++){
             MyTask myTask = new MyTask(i);
             executor.execute(myTask);
             System.out.println("线程池中线程数目："+executor.getPoolSize()+"，队列中等待执行的任务数目："+
             executor.getQueue().size()+"，已执行玩别的任务数目："+executor.getCompletedTaskCount());
         }
         executor.shutdown();
     }
}
 
 
class MyTask implements Runnable {
    private int taskNum;
     
    public MyTask(int num) {
        this.taskNum = num;
    }
     
    @Override
    public void run() {
        System.out.println("正在执行task "+taskNum);
        try {
            Thread.currentThread().sleep(4000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println("task "+taskNum+"执行完毕");
    }
}
```

2. ## ThreadPoolExecutor类的参数有哪些

   + corePoolSize

     核心池的大小

   + maximumPoolSize

     线程池最大线程

   + keepAliveTime

     示线程没有任务执行时最多保持多久时间会终止。默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会起作用，直到线程池中的线程数不大于corePoolSize

   + unit

     参数keepAliveTime的时间单位

   + workQueue

     一个阻塞队列，用来存储等待执行的任务

     常用的队列有如下几个：

     ArrayBlockingQueue:基于数组的先进先出队列，此队列创建时必须指定大小；

     LinkedBlockingQueue:基于链表的先进先出队列，如果创建时没有指定此队列大小，则默认为Integer.MAX_VALUE；

     SynchronousQueue:这个队列比较特殊，它不会保存提交的任务，而是将直接新建一个线程来执行新来的任务

     

   + threadFactory

     线程工厂，主要用来创建线程

   + handler

     表示当拒绝处理任务时的策略，有以下四种取值

     ```java
     ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 
     ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 
     ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）
     ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 
     ```

3. 线程池的状态

   线程池有4种状态:RUNNING、SHUTDOWN、STOP、TERMINATED

   创建线程后，初始时，线程池处于RUNNINING状态；

   如果调用了shutdown()方法，则线程池处于SHUTDOWN状态，此时线程池不能够接受新的任务，它会等待所有任务执行完毕；

   如果调用了shutdownNow()方法，则线程池处于STOP状态，此时线程池不能接受新的任务，并且会去尝试终止正在执行的任务；

   当线程池处于SHUTDOWN或STOP状态，并且所有工作线程已经销毁，任务缓存队列已经清空或执行结束后，线程池被设置为TERMINATED状态；

4. 任务提交给线程池之后的处理策略

   - 如果当前线程池中的线程数目小于corePoolSize，则每来一个任务，就会创建一个线程去执行这个任务；
   - 如果当前线程池中的线程数目>=corePoolSize，则每来一个任务，会尝试将其添加到任务缓存队列当中，若添加成功，则该任务会等待空闲线程将其取出去执行；若添加失败（一般来说是任务缓存队列已满），则会尝试创建新的线程去执行这个任务；
   - 如果当前线程池中的线程数目达到maximumPoolSize，则会采取任务拒绝策略进行处理；
   - 如果线程池中的线程数量大于 corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止，直至线程池中的线程数目不大于corePoolSize；如果允许为核心池中的线程设置存活时间，那么核心池中的线程空闲时间超过keepAliveTime，线程也会被终止

5. 线程池的关闭

   + shutdown()

     不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务

   + shutdownNow()

     立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务

6. xx



### 1.2.4 future









### 1.2.5 StringBuilder、StringBuffer与String的区别的区别

1. StringBuffer是线程安全的，StringBuilder不是线程安全的
2. 缓冲区的区别：StringBuffer每次获取toString都会直接使用缓冲区的toStringCache值来构造一个字符串，而StringBuilder则每次都需要复制一次字符数组，再构造一个字符串
3. 性能:StringBuilder的性能远优于StringBuffer
4. 三者的区别：String是不可变的，另外两者都是可变的



三者的底层实现也需要掌握





### 1.2.6 进程、线程、协程的区别和联系



进程满足不了哪些才出现了线程



### 1.2.7 java的垃圾回收算法





### 1.2.8 LinkedList和ArrayList的区别







### 1.2.9 java什么地方用到了红黑树

红黑树是什么





### 1.2.10 sleep和wait的区别







### 1.2.11 Volatile关键字是怎么保证线程安全的



从java内存模型的角度谈一谈？

与synchronized的区别



### 1.2.12 讲一下JVM对于synchronized的优化

锁升级





### 1.2.13 讲一下锁粗化







### 1.2.14 JVM的内存区域都有哪些







### 1.2.15 什么情况下会导致OOM

和堆里的OOM有什么区别

哪个区域不会发生OOM





### 1.2.16 讲一下反射

什么场景下会用到反射





### 1.2.17 cpu一直100%的原因分析

如果我发现当前系统的的CPU利用率一直是100%，可以讲一下造成这个现象的原因吗

如何使用java命令结合linux命令找出问题所在





### 1.2.18 衡量java代码运行效率的指标







### 1.2.19 用通俗的语言介绍下多态







### 1.2.20 JMM





### 1.2.21 重载和重写的区别

英文名称分别是什么





### 1.2.22 ConcurrentHashMap

底层实现



### 1.2.23 equals 和==操作符的区别

equals和hashcode之间的关系





### 1.2.24 精度问题

两个double相加，比如0.1+0.1=0.200001这类问题如何解决









### 1.2.25 JVM相关

面试官问：我main方法new 了一个类，并调用它的run方法，JVM中都发生了什么







### 1.2.26 强引用、弱引用、虚引用





### 1.2.27 ThreadLocal可能导致的内存泄漏问题





### 1.2.28 JVM中的锁







### 1.2.29 lock和synchronized的区别







### 1.2.30 除了锁，其他保证线程安全的方式







### 1.2.31 同步块外部执行wait和notify能达到效果吗





### 1.2.32 java concurrent包里面用过什么



### 1.2.33 读写锁

为什么并发读要加锁





### 1.2.34 使用synchronized关键字有什么隐患





### 1.2.35 如何终止一个正在运行的线程







### 1.2.36 JVM CMS垃圾回收器和G1之间的区别？

G1详细是怎么回收垃圾的



### 1.2.37 JVM如何判断对象是否可回收







### 1.2.38 什么时候、什么情况下会发生GC回收

新生代和老年代分别介绍





### 1.2.39 java调试命令

看线程运行状态用什么？

看堆栈信息用什么





### 1.2.40 反射机制













### 1.2.41 抽象类和接口

















## 1.3 Scala





# 2.算法方面

## 2.1 各种排序算法代码实现以及空间时间复杂度(重点堆排序和快排)

如何理解排序的稳定性

哪些是稳定的？哪些不稳定









## 2.2 有序数组求两个数的差值绝对值最大数







## 2.3 二叉树镜像





## 2.4 两个无序数组合并后冒泡





## 2.5重拍链表





[重排链表](https://www.nowcoder.com/jump/super-jump/word?word=重排链表)：1->2->3->4->5->6->7  结果: 1->7->2->6->3->5->4







## 2.6 逆序对





## 2.7 无限的水

3升跟 5升的桶，怎么装出 4升水





## 2.8 划分字段



根据一系列分隔符划分字段，可以用正则分布





## 2.9 数组最小和问题



定义：一个数组从0~N-1，每个元素的左侧小于或等于该元素的所有元素之和称之为该元素的最小和。数组中所有元素的最小和称之为数组的最小和。求给定数组的最小和。



## 2.10 单链表翻转







## 2.11 原地删除有序数组中重复的元素

原地删除有序数组中重复的元素，然后返回删除后的长度







## 2.12 随机快排





## 2.13 TopK





## 2.14 找出只出现一次的数



一个数组中，只有一个数字出现了一次，其它的都出现了两次，请你找出那个只出现一次的数字，比较简单，数组中的所有数全部异或一下就可以了





## 2.15 无序数组求目标值

hash





## 2.16 大数据算法

paxos,2pc,3pc,zab,cap,base





## 2.17 贪心算法和动态规划有什么区别





## 2.18 最大连续子数组和





## 2.19 逆序栈中数据

不使用其他数据结构，只用一个辅助栈和递归算法，逆序栈中数据





## 2.20 数组中第K大的数

给定一个整数数组a,同时给定它的大小n和要找的K(K在1到n之间)，请返回第K大的数，保证答案存在



## 2.21 和为0的三元组



给出一个有n个整数的数组S，在S中找到三个整数a, b, c，找到所有使得a + b + c = 0的三元组



## 2.22 机器人走步问题

有一个机器人的位于一个 m × n 个网格左上角。
机器人每一时刻只能向下或者向右移动一步。机器人试图达到网格的右下角。
问有多少条不同的路径





## 2.23 八皇后问题



## 2.24 连续子数组的最大值









## 2.25 小于num的平方根的最大整数





## 2.26 一个表里面存放了父节点与子节点的结构

怎么遍历



​	





































# 3.Spark

## 3.1 DAG生成和Stage划分

### 3.1.1 什么是DAG

DAG:有向无环图，指的是数据转换执行的过程，有方向，无闭环，原始的RDD通过一系列的转换操作就形成了DAG有向无环图，任务执行时可以按照DAG的描述执行真正的计算。

有以下几个注意点:

1. 一个Spark应用中可以有一到多个DAG,取决于触发了多少次Action
2. 一个DAG中会有不同的阶段/stage,划分阶段/stage的依据是宽依赖
3. 一个阶段/stage中可以有多个Task,一个分区对应一个Task

### 3.1.2 如何划分DAG的stage

对于窄依赖，partition的转换处理在stage中完成计算，不划分(将窄依赖尽量放在在同一个stage中，可以实现流水线计算)

对于宽依赖，由于有shuffle的存在，只能在父RDD处理完成后，才能开始接下来的计算，也就是说需要要划分stage（出现宽依赖即拆分）



stage切割规则：从后往前，遇到宽依赖就切割stage



提交的整个spark任务为一个application,根据任务里面的action算子可以将application划分为多个Job,每个job按照宽依赖划分为多个stage，每个stage按照处理数据不同(默认numslice数目)划分为不同的task

### 3.1.3 为什么要划分Stage

概括来将是便于并行计算

一个复杂的业务逻辑如果有shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，即下一个阶段的计算要依赖上一个阶段的数据。那么我们按照shuffle进行划分(也就是按照宽依赖进行划分)，就可以将一个DAG划分成多个Stage/阶段，在同一个Stage中，会有多个算子操作，可以形成一个pipeline流水线，流水线内的多个平行的分区可以并行执行


## 3.2 宽窄依赖

窄依赖定义：窄依赖表示每一个父(上游)RDD 的 Partition 最多被子（下游）RDD 的一个 Partition 使用

宽依赖定义：宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle



窄依赖有3种:

+ 一对一依赖
+ Range依赖
+ Prune依赖

宽依赖只有一种:shuffle依赖

## 3.3 造成shuffle的一些操作

> Operations which can cause a shuffle include **repartition** operations like [`repartition`](http://spark.apache.org/docs/latest/rdd-programming-guide.html#RepartitionLink) and [`coalesce`](http://spark.apache.org/docs/latest/rdd-programming-guide.html#CoalesceLink), **ByKey** operations (except for counting) like [`groupByKey`](http://spark.apache.org/docs/latest/rdd-programming-guide.html#GroupByLink) and [`reduceByKey`](http://spark.apache.org/docs/latest/rdd-programming-guide.html#ReduceByLink), and **join** operations like [`cogroup`](http://spark.apache.org/docs/latest/rdd-programming-guide.html#CogroupLink) and [`join`](http://spark.apache.org/docs/latest/rdd-programming-guide.html#JoinLink).

上面试官方给出的，总结一下就是如下操作会导致shuffle:

1. 类似repartition和coalesce这样的repartition操作
2. 类似groupByKey和reduceByKey这样的除了用于counting的ByKey操作
3. 类似cogroup和join这样的join操作



## 3.4 repartition和coalesce的关系

通过 coalesce 方法，收缩合并分区，减少分区的个数,repartition内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true。无论是将分区数多的RDD 转换为分区数少的 RDD，还是将分区数少的 RDD 转换为分区数多的 RDD，repartition操作都可以完成，因为无论如何都会经 shuffle 过程



## 3.5 常见的action函数

下面的图片来自官网

![image-20210407181253882](images/image-20210407181253882.png)



## 3.6 常见的transformation函数

下面的图片来自官网

![img](https://img2018.cnblogs.com/blog/798311/201907/798311-20190724191514679-2000017236.png)





## 3.7 wordCount代码

### 3.7.1 scala版

```scala
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}
 
object partice1{
 
  def main(args: Array[String]): Unit = {
 
    val conf = new SparkConf()
 
    /**
      * 如果这个参数不设置，默认运行的是集群模式
      * 如果设置成local代表运行的是local模式
      */
    conf.setMaster("local")
    //设置任务名
    conf.setAppName("WordCount")
    //创建SparkCore的程序入口
    val sc = new SparkContext(conf)
    //读取文件  生成RDD
    val file: RDD[String] = sc.textFile("D:\\hello.txt")
    //把每一行数据按照","分割
    val word: RDD[String] = file.flatMap(_.split(","))
    //让每一个单词都出现一次
    val wordOne: RDD[(String, Int)] = word.map((_,1))
    //单词计数
    val wordcount: RDD[(String, Int)] = wordOne.reduceByKey(_+_)
    //按照单词出现的次数  降序排序
    val sortRDD: RDD[(String, Int)] = wordcount.sortBy(tuple => tuple._2,false)
    //将最终的结果进行保存
    sortRDD.saveAsTextFile("D:\\aaa")
 
    sc.stop()
  }
}
```

### 3.7.2 java版

```java
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;
 
import java.util.Arrays;
 
public class SparkWordCountWithJava8 {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf();
        conf.setAppName("WortCount");
        conf.setMaster("local");
        JavaSparkContext sc = new JavaSparkContext(conf);
 
        JavaRDD<String> fileRDD = sc.textFile("E:\\hello.txt");
        JavaRDD<String> wordRdd = fileRDD.flatMap(line -> Arrays.asList(line.split(",")).iterator());
        JavaPairRDD<String, Integer> wordOneRDD = wordRdd.mapToPair(word -> new Tuple2<>(word, 1));
        JavaPairRDD<String, Integer> wordCountRDD = wordOneRDD.reduceByKey((x, y) -> x + y);
        JavaPairRDD<Integer, String> count2WordRDD = wordCountRDD.mapToPair(tuple -> new Tuple2<>(tuple._2, tuple._1));
        JavaPairRDD<Integer, String> sortRDD = count2WordRDD.sortByKey(false);
        JavaPairRDD<String, Integer> resultRDD = sortRDD.mapToPair(tuple -> new Tuple2<>(tuple._2, tuple._1));
        resultRDD.saveAsTextFile("E:\\result8");
 
    }
```



## 3.8 Spark为什么快

1. 消除了冗余的HDFS读写

   Hadoop每次shuffle操作后，必须写到磁盘，而Spark在shuffle后不一定落盘，可以cache到内存中，以便迭代时使用。如果操作复杂，很多的shufle操作，那么Hadoop的读写IO时间会大大增加

2. 消除了冗余的MapReduce阶段

   Hadoop的shuffle操作一定连着完整的MapReduce操作，冗余繁琐。而Spark基于RDD提供了丰富的算子操作，且reduce操作产生shuffle数据，可以缓存在内存中。

3. JVM的优化

   Spark Task的启动时间快。Spark采用fork线程的方式，Spark每次MapReduce操作是基于线程的，而Hadoop采用创建新的进程的方式，启动一个Task便会启动一次JVM，Spark的Executor是启动一次JVM，内存的Task操作是在线程池内线程复用的，每次启动JVM的时间可能就需要几秒甚至十几秒，那么当Task多了，这个时间Hadoop不知道比Spark慢了多



还有一种问法：

spark在集群中怎么执行程序的？如果有聚合操作呢？

​		

## 3.9 任务执行流程

1. 构建DAG（调用RDD上的方法）
2. DAGScheduler将DAG切分Stage（切分的依据是shuffle），将Stage中生成的Task以TaskSet的形式给TaskScheduler
3. TaskScheduler调度Task（根据资源情况将Task调度到相应的Executor中）
4. Executor接收Task，然后将Task丢入到线程池中执行
   
   

## 3.10 flink和spark streaming有什么区别

主要从2个方面来回答：

1. 数据模型
   - spark采用RDD模型，spark streaming的DStream实际上也就是一组组小批数据RDD的集合
   - flink基本数据模型是数据流以及事件(event)序列
2. 运行时架构
   - spark是批计算，将DAG划分为不同的stage，一个完成后才可以计算下一个
   - Flink是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节点进行处理
3. 容错
   - Flink基于两阶段提交实现了精确一次语义
   - spark streaming只能做到不丢数据但是有重复
4. 



## 3.11 spark streamiing相比flink有什么优点





## 3.12 调优





## 3.13 Spark的RDD讲一下



## 3.14 spark和mr的区别







## 3.15 spark map和flatmap的区别



































# 4. Redis(√)

## 4.1 Redis支持哪些数据结构(5种)

+ String
+ Hash
+ List
+ Set
+ Zset



## 4.2 redis HA架构

Redis Sentinel 是一个分布式系统， 你可以在一个架构中运行多个 Sentinel 进程（progress）， 这些进程使用流言协议（gossip protocols)来接收关于主服务器是否下线的信息， 并使用投票协议（agreement protocols）来决定是否执行自动故障迁移， 以及选择哪个从服务器作为新的主服务器

> gossip 协议（gossip protocol）又称 epidemic 协议（epidemic protocol），是基于流行病传播方式的节点或者进程之间信息交换的协议
>
> 

Redis 的 Sentinel 系统用于管理多个 Redis 服务器（instance）， 该系统执行以下三个任务：

- **监控（Monitoring**）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。
- **提醒（Notification）**： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。
- **自动故障迁移（Automatic failover）**： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时， 集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。





## 4.4 跳表

### 4.4.1 什么是跳表

跳表是在单链表基础上加入索引，查找、插入、删除时间复杂度都是O(logn)的数据结构

### 4.4.2 跳表java实现

```java
/**
 * 跳表是在单链表的基础上加入索引，使查找效率大大提高的一种各方面性能都很优秀的数据结构
 */
public class mySkipList {
    private final int MAX_LEVEL = 16;//最大索引层数（0~原链表 15~最高一级索引）
    private int levelCount = 1;//跳表当前索引层数
    public Node head = new Node();//跳表头

    /**
     * 查找跳表中值为value的节点
     *
     * @param value 要查找的值
     * @return 找到返回对应节点，否则返回null
     */
    public Node find(int value) {
        Node p = head;
        //找到该层索引中小于value的最大节点
        for (int i = levelCount - 1; i >= 0; i--) {
            while (p.forwards[i] != null && p.forwards[i].data < value) {
                p = p.forwards[i];
            }
        }

        if (p.forwards[0] != null && p.forwards[0].data == value)
            return p.forwards[0];
        else
            return null;
    }

    /**
     * 将value插入跳表中
     *
     * @param value 待加入数据
     */
    public void insert(int value) {
        int level = randomLevel();
        if (levelCount < level) levelCount = level;

        Node p = head;
        Node newNode = new Node();
        newNode.data = value;
        newNode.maxLevel = level;

        Node path[] = new Node[level];//存储查找value时经过各层的索引
        for (int i = level - 1; i >= 0; i--) {
            while (p.forwards[i] != null && p.forwards[i].data < value) {
                p = p.forwards[i];
            }
            path[i] = p;
        }

        //将value插入各层索引中
        for (int i = level - 1; i >= 0; i--) {
            newNode.forwards[i] = path[i].forwards[i];
            path[i].forwards[i] = newNode;
        }
    }

    /**
     * insert的优化版本，去掉了path[]
     * @param value 待加入数据
     */
    public void insert_optimized(int value) {
        int level = randomLevel();
        if (levelCount < level) levelCount = level;

        Node p = head;
        Node newNode = new Node();
        newNode.data = value;
        newNode.maxLevel = level;

        for (int i = level - 1; i >= 0; i--) {
            while (p.forwards[i] != null && p.forwards[i].data < value)
                p = p.forwards[i];

            //这层索引是最后一个则直接插入
            if (p.forwards[i] == null) {
                p.forwards[i] = newNode;
            }
            //否则插在中间
            else {
                newNode.forwards[i] = p.forwards[i];
                p.forwards[i] = newNode;
            }
        }
    }

    /**
     * 删除跳表中值为value的节点及索引
     *
     * @param value 待删除结点的值
     */
    public void delete(int value) {
        Node path[] = new Node[levelCount];
        Node p = head;

        for (int i = levelCount - 1; i >= 0; i--) {
            while (p.forwards[i] != null && p.forwards[i].data < value)
                p = p.forwards[i];
            path[i] = p;
        }

        //找到
        if (p.forwards[0] != null && p.forwards[0].data == value) {
            //删除节点所有索引
            for (int i = levelCount - 1; i >= 0; i--) {
                if (p.forwards[i] != null && p.forwards[i].data == value) {
                    p.forwards[i] = p.forwards[i].forwards[i];
                }
            }
        }
    }

    /**
     * 随机生成索引层数，索引层数和生成概率负相关
     * 尽量使一级索引占全部索引的50%，二级索引占25%，三级索引占12.5%……
     * 随机函数能放置数据全部集中在某两个索引之间
     *
     * @return
     */
    private int randomLevel() {
        int level = 1;

        while (Math.random() < 0.5 && level < MAX_LEVEL)
            level++;

        return level;
    }

    public void printAll() {
        Node p = head;
        while (p.forwards[0] != null) {
            System.out.print(p.forwards[0].data + " ");
            p = p.forwards[0];
        }
        System.out.println();
    }

    public void skipListText() {
        mySkipList list = new mySkipList();
        list.insert(1);
        list.insert(2);
        list.insert(3);
        list.printAll();
        System.out.println(list.find(2));
        list.delete(2);
        list.printAll();
        list.insert_optimized(2);
        list.printAll();
    }

    public class Node {
        private int data = -1;//节点数据
        private Node forwards[] = new Node[MAX_LEVEL];//存储节点上层索引
        private int maxLevel = 0;//最大索引层数

        @Override
        public String toString() {
            StringBuilder builder = new StringBuilder();

            builder.append("{ data: ");
            builder.append(data);
            builder.append("; levels: ");
            builder.append(maxLevel);
            builder.append(" }");

            return builder.toString();
        }
    }
}
```







# 5. HDFS

## 5.1 HDFS架构

要求能把图画出来并且说出来



+ NameNode

  一个集群里只有一个，存在单点问题

  主要存放元数据信息，包括谁创建的、权限、文件对应的block信息

+ DataNode

  一个集群中有多个

  存储数据

  与NameNode之间有心跳

+ SecondaryNameNode





## 5.2 HDFS读写流程



### 5.2.1 写流程

可能的问法：

我现在要往HDFS上put一个文件，你说说HDFS上都干了什么



1. client从配置文件或者调用client的代码中获得必要的参数，例如blocksize/文件副本数
2. 根据1中获取的参数将待上传文件拆分成相应数量的block(这里注意切分block是client做的)
3. client提交参数到NameNode
4. NameNode 向client返回文件存放的DataNode(比如说是DataNode 1 2 3)
5. client请求向DataNode1写入数据，DataNode1在接收客户端数据的同时以packet为单位将数据复制到NN2 3上，当完成一个block的写入后，通知NameNode

![image-20210407170230502](images/image-20210407170230502.png)

### 5.2.2 读流程

1. client请求NameNode获取目标文件的元数据信息
2. NameNode将目标文件的元数据信息返回给client
3. client根据NameNode返回的目标文件的元数据信息请求对应的DataNode获取相应的block
4. 如果目标文件有多个block，将多个block合并





## 5.3 HDFS HA



这个地方联想到yarn的HA是怎么实现的



![image-20210407170813310](images/image-20210407170813310.png)





## 5.4  小文件是什么

### 5.4 1 小文件定义





### 5.4.2为什么会有小文件





## 5.5 小文件给Hadoop集群带来的瓶颈问题





## 5.6 副本存放策略

这个在之前学习Hadoop HDFS的笔记上应该可以找到答案





## 5.7 Hadoop 1.0和2.0 HDFS的block各为多少



## 5.8 如何保证数据不丢失







## 5.9 数据本地性











# 6. Hadoop

## 6.1 Hadoop 1.0和2.0的区别







## 6.2 NameNode的HA







## 6.3 槽位数的共享问题

需要注意什么

reduce slot提前启动和map slot饿死





## 6.4 Haoop2.0如何进行资源调度







## 6.5 Hadoop的介绍









## 6.6 MP的shuffle过程讲一下











## 6.7 高可用

两个NameNode如何保持同步

脑裂和防止脑裂





## 6.8 SecondaryNameNode的作用



















# 7.Hive

## 71. MapReduce 和 Hive的区别





## 7.2 外部表和内部表







## 7.3 优化

表级别的优化









## 7.4 开窗HQL

给定一个表，有学期，学号，课程号，成绩，求连续两个学期都选了数学的学生



## 7.5 连续登陆HQL

思路要记住





## 7.6 hive使用过程中遇到了什么问题





可以回答数据倾斜





## 7.7 hive处理逻辑的时候遇到过什么难点







## 7.8 开窗函数有哪些





rank、dense_rank、row_number有什么区别以及应用场景











## 7.9 计算引擎

mr

tez

spark

tez引擎的优点









## 7.10 自定义UDF,UDTF



## 7.11 HQL题目



### 7.11.1 统计前两名

省份，城市，人口数量，统计人口排名前两名城市



### 7.11.2 最大在线人数

给定uid,login_time,logout_time 求每分钟最大的在线人数.两个time时间都是标准时间datetime(2019-01-01 12:00:00)这种.说实话这个挺难的,我的直觉想法就是用UDF/transform来解决.面试官肯定了解法,但是 不是想要听到的答案.他说出这道题的目的就不是让你能用SQL写出来的.







## 7.12 模糊去重和精确去重的算法





## 7.13 map和reduce数怎么确定







## 7.14 order by 和 sort by的执行区别







## 7.15 hive和hbase的区别

使用场景 







## 











# 8.flume

## 8.1 常用的source



## 8.2 常用的channel





## 8.3 常用的sink







## 8.4 拦截器



自定义拦截器



## 8.5 小文件处理









# 9.Hbase

## 9.1 rowkey的选择





## 9.2 rowkey的过长或过热问题









## 9.3 删除数据





## 9.4 hbase为何能海量存储，oracle为什么不行





## 9.5 hbase底层以什么形式存储在HDFS上





## 9.6 HBase和HDFS的区别







## 9.7 与mySQL的区别



取代的可能性与场景







## 9.8 列式存储的特点













## 9.9 整体架构





## 9.10 region分裂

怎么分裂的？

父region是直接删掉了吗







# 10.sqoop



# 11.ElasticSearch

## 11.1 倒排索引





## 11.2 ES查询到很多数据该怎么返回

















# 12. MySQL

## 12.1 索引为何选B树，B+树不选二叉树





## 12.2 事务的ACID





## 12.3 传统关系型数据库和非关系型数据库HBase的区别





## 12.4 数据库数据改变后怎么获取哪些数据改变了，哪些没变





## 12.5 默认

默认的执行引擎是什么

默认的隔离级别是什么







## 12.6 MVCC





实现原理













## 12.5 范式

范式以及举例







## 12.6 表设计



根据用户下单商家接单配送员配送这么一个场景，自己设计表和字段，表间关联，以及是什么类型的表







## 12.7 隔离级别



## 12.8 什么是索引

有哪些索引

索引的缺点

B树和B+树有什么优缺点

索引底层结构

插入数据后索引结构的变化

索引的优化方法







## 12.9 DDL/DML

数据库查询语言、数据库操纵语言、数据库定义语言、数据库控制语言





## 12.10 Drop、truncate、delete的区别







## 12.11 题目

### 12.11.1 交易额筛选

一张城市和交易额表，一张城市对应省份表， 取出 省份 总 交易额大于 500 的 省份 的名字



基于上面 得出的 省份 总 交易额 [0,500 ] , [500,1000 ] , [1000,+oo ] 在以下三个区间的 省份 的 数量



还是基于上面， 按从小到大的顺序得出每个城市的累计交易额，可以用窗口



### 12.11.2 join

A,B表都只有id = {1,2,3},写出下面两个语句的输出结果

![img](https://uploadfiles.nowcoder.com/images/20210329/936421974_1617026876816/879CD6FAD433E1642FBB9DA24ED5B8CB)







## 12.12 最左匹配原则





## 12.13 数据怎么建索引







## 12.14 什么是事务



事务的实现原理













## 12.11 表连接的类型



inner/left/right/full join

以 left join为例说一下特点







## 12.12 锁有哪些



间隙锁



## 12.13 聚簇索引





## 12.14 group by实现原理







## 12.15 脏读、幻读、不可重复读







## 12.16 两个引擎的区别



## 12.17 索引失效的情况有哪些









## 12.18 两个表使用where和on的区别







## 12.19 数据库几种日志

如何刷新？区别？



## 12.20 数据库3个线程用处









## 12.21 一条SQL语句执行的慢的原因





## 12.22 可有哪些方式来优化SQL











# 13. 项目架构

## 13.1 项目里面怎么离线处理离线数据的







## 13.2 项目的数据量有多大







## 13.3 基本磁盘离线处理和基于内存处理海量数据的区别





## 13.4 设计一个数据系统





## 13.5 知道哪些nosql数据库



















## 

















# 14. 设计模式

重点掌握原理以及代码实现

面试问到：实现观察者模式

类型特点应用场景







# 15. 网络

## 15.1 TCP/IP/UDP





## 15.2 http和https





## 15.3 cookie和session







## 15.4 http位于协议模型的哪一层





## 15.5 post和get的区别





## 15.6 http的状态码

状态码有哪些，分别代表什么意思





## 15.7 tcp 三次握手

为什么是3次，两次可以吗





## 15.8 TCP可靠性怎么保证







## 15.9 拥塞控制的4个阶段





## 15.10 流量控制的方法

为了解决什么问题



流量控制中的死锁问题?







## 15.11 四次挥手









## 15.12 从发送请求到获得网页内容的全过程











# 16.zookeeper

## 16.1 leader选择算法









# 17. 数仓

## 17.1 对数仓有什么了解





## 17.2 数仓和数据库的区别







## 17.3 数仓的表

实体表

维度表

事实表

怎么理解维度表





## 17.4 数仓维度模型

介绍星型模型和雪花模型的区别以及应用场景



## 17.5 项目全流程



把尚硅谷公开课总结的那张流程图记下来





## 17.6 数仓分层依据





## 17.7 维度建模

维度建模做过什么

维度建模的方法论

维度建模过程





## 17.8 数据倾斜

是什么

怎么解决



## 17.9 数据入仓





## 17.10 增量合并怎么实现的







## 17.11 如何解决小文件问题





## 17.12 产品的指标如何知道给用户带来多少价值





## 17.13 指标如何做到精准





## 17.14 就[项目](https://www.nowcoder.com/jump/super-jump/word?word=项目)中提到的某个实体，讲一下他对应的属性有哪些







## 17.15 如果你建模的话，你会如何建模





## 17.16 主要都抽取哪些数据源，使用什么工具







## 17.17 描述一下抽取的内部逻辑，怎么实现的





## 17.18 如何保证数据质量，数据一致性？







## 17.19 拉链表

拉链表了解吗？

什么情况下会用到

怎么实现





## 17.20 缓慢维度变化问题





支架表？拉链表？

统计带状维度如何更新



为什么维度要做到扁平多对一



元数据怎么管理



















# 18. 数据结构



## 18.1 常见的数据结构

堆、红黑树 数之类的





## 18.2 链表和数组的区别



什么时候用数组，什么时候用链表



数组和链表的特点

## 18.3 队列跟栈的区别





## 18.4 怎么找到两个链表的交点

时间复杂度是多少

双指针把两个链表都走一遍，两个指针相遇的点就是交点

除了上面的方法还有其他方法吗

如果两个链表有多个交叉点该怎么找





## 18.5 二叉树的种类



完全二叉树的作用，其他数的作用也需要了解









## 18.6 B树和B+树的区别





## 18.7 位图







## 18.8 Bloom过滤器











# 19. 逻辑问题

## 19.1 扔硬币

逻辑题：可以无限次扔硬币，得到最后三次为 “正反反”或 “反反正”则结束扔硬币。问以“正反反”和 “反反正”结束的概率，哪个概率大些？

回答的 正反反，但理由分析不太对，没说清楚。





## 19.2 哪个杂质多

两个烧杯，一个放糖一个放盐，用勺子舀一勺糖到盐，搅拌均匀，然后舀一勺混合物放回糖的烧杯，问你两个烧杯哪个杂质多



## 19.3 开关

有两个房间，一间房间里有三盏灯，另一个房间里有控制这三盏灯的三个开关（这两个房间是分割开的，毫无联系）.现在你分别进入这两个房间一次，然后判断出这三盏灯分别是由哪个开关控制的





## 19.4 找出重量不一样的球

12个球，其中有1个坏球和其他11个重量不一样，给你一个天平，称3次，找出不一样的那个

球的数量是8个呢？是否有区别？











# 20. 操作系统



## 20.1 死锁



什么是死锁

怎么解决死锁

用生活中的例子解释一下死锁





死锁的4个必要条件



## 20.2 进程间的通信方式







## 20.3 进程间的调度算法以及优缺点







## 20.4 内存管理方式





## 20.5 操作系统打开文件到屏幕显示内容的过程

















# 21. linux

## 21.1 查找文件的命令





## 21.2 5种IO模型





## 21.3 常用命令





## 21.4 shell相关

$0表示什么意思



## 21.5 监控cpu的命令

top



## 21.6 监控端口的命令

netstat















# 22. 分布式

## 22.1 讲一下CAP



## 22.2 分布式存储系统和分布式计算框架区别













# 23. MapReduce

## 23.1 MapReduce的10个步骤







## 23.2 MapReduce的优化







## 23.3 MapReduce里面哪里磁盘IO最大



## 23.4 手写MapReduce代码





## 23.5 说一下shuffle





## 23.6 自定义分区





## 23.7 支持的压缩

每个的特点需要知道



## 23.8 原理



## 23.9 详细过程



注意详细





## 23.10 架构

需要能画出来并且说出来







# 24. Kafka

## 24.1 可靠性怎么实现的





## 24.2 一致性如何实现





## 24.3 leader选举机制





## 24.4 ISR列表





## 24.5 副本复制机制





## 24.6 acks参数



## 24.7 架构





## 24.8 发送数据、消费数据





## 24.9 高吞吐





## 24.10 作用











# 25. Yarn

## 25.1 资源调度流程和调度器





## 25.2 容错机制





# 26. Flink

## 26.1 窗口函数





## 26.2 一次性语义













# 27. java系列框架

## 27.1 Spring和Spring Boot的区别



## 27.2 Spring Boot的缺陷是什么





## 27.3 XML配置和注解配置各自的优缺点































































